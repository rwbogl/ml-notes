# Reinforcement Learning

For every reinforcement problem, we must have the following things:

- __State space__:
    - set of states that an agent may be in
    - information that an agent has access to
    - environment information, if it is changing

- __Action space__:
    - set of _all_ possible actions
    - the state space may encode all _possible_ actions from a given state

(We would, in general, like to minimize the size of these spaces.)

- __Reward function__:
    - set by the environment
    - returns an _actual_ reward for every action taken

- __Discounting__:
    - subjective reward function that attempts to predict future rewards from
      an action
    - Choose $\gamma \in [0, 1)$, a measure of how much we believe our
      prediction.
    - Our estimated reward at time $t$ is weighted by $\gamma^t$.

## Action Selection

Every agent must have a way to select an action at each state. To assist in
this, $Q_{s, t}(a)$ is defined as the average reward for choosing action $a$ at
state $s$ after choosing it $t$ times in the past. We would like for $Q_{s,
t}(a)$ to converge to the _actual_ reward for the action.

We can think of $Q_{s, t}(a)$ as a three dimensional table; or as the continual
replacement of a two dimensional table as $t$ increases.

There are various ways to choose the next action:

- __Greedy__: choose the action $a$ that maximizes $Q_{s, t}(a)$.

- __$\epsilon$-Greedy__: generally choose the greedy solution, but with
  $\epsilon$ probability, uniformly choose a random action.

- __Softmax__: Choose an action relative to the softmax probabilities; i.e. set
  $$P(Q_{s, t}(a)) = \frac{\exp(Q_{s, t}(a) / \tau)}{\sum_{k} \exp(Q_{s, t}(k)
  / \tau)}$$ and choose action $a$ relative to this probability^[That is, pick a
  random number $r$. If $r$ is less $P(Q_{s, t}(1)$, then choose action 1. If
  $r$ is larger than it, then examine the second action. And so on until the
  last action is reached.].

RL problems can be split into two classes: episodic and continual. An
_episodic_ problem has a definite goal to be reached. A _continual_ problem has
no definite goal, and goes on indefinitely.

Clearing up the example:

- The agent does _not_ know the overall topography.

- Absorbing states have no actions to choose from.

## Values

The explanation of values is a little abstruse. Roughly, values are the reward
that an agent expects to get from an action at a certain state. That is, they
are a "subjective reward." The goal of reinforcement learning is to find a
policy of choosing actions that will maximize these subjective values.

This policy will be created by initializing all values to small, random
numbers, then exploring the state space. As the agent explores the state space,
it uses the rewards it obtains to update the values it believes each action and
state should have. With any luck, the values will converge to the actual
rewards.

There are two different value functions:

- __State value function__: $$V(s) = E[r_t \mid s_t] = E[\sum_{k = 0}^\infty
  \gamma^k r_{t + k + r} \mid s_t = s].$$
    - Let current policy set action; average over all actions.

- Action-value function: $$Q(s, a) = E[r_t \mid s_t, a_t] = E[\sum_{k =
  0}^\infty \gamma^k r_{t + k + 1} \mid s_t = s, a_t = a]$$
    - Let current policy select action; take the value from only those actions.

The optimal value function $Q^*$ is given by the authors as $$Q^*(s_t, a_t) =
E[r_{t + 1}] + \gamma \max_{a_{t + 1}} Q(s_{t + 1}, a_{t + 1}).$$ This doesn't
seem to build up to much. Later, they give an update formula $$Q(s, a)
\leftarrow Q(s, a) + \mu(r + \gamma Q(s', a') - Q(s, a)).$$

## The Algorithms

Q-Learning and Sarsa do roughly the same thing. The only difference is that
Sarsa uses the "current policy" to make _all_ of its decisions, where
Q-Learning uses $\epsilon$-greedy for some decisions.
