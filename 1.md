# Introduction

The introduction of Section 1.2 is dense, but contains important points.
Machine learning generally tries to emulate the following process:

- _Remembering_ past examples.
- _Adapting_ memory of past examples for the present.
- _Generalizing_ knowledge to handle new examples.

The four main types of learning algorithms are summarized in a list in Section
1.3, reproduced here:

- _Supervised learning_, where training inputs have target outputs.
- _Unsupervised learning_, where inputs do not have target outputs.

- _Reinforcement learning_, where an evaluator is told when they are wrong, but
  not the answer.

- _Evolutionary learning_, where random solutions are generated to maximize
  some fitness function.

These classifications are slightly arbitrary. Reinforcement learning and the
fitness function in evolutionary learning are both very much like supervised
learning.

A rough sketch of the overall learning process is given in Section 1.5,
reproduced here:

1. Data Collection and Preparation
2. Feature Selection
3. Algorithm Choice
4. Parameter and Model Selection
5. Training
6. Evaluation

No learning takes place until the Training step. The steps before it are
entirely focused on finding data and tailoring the learning algorithm choice to
the problem at hand.

Section 1.6 ("A Note on Programming") could be important. Random programs
aren't reproducible, so the `np.random.seed()` call is necessary for debugging
specific cases. Aside from this method of debugging, reference programs are
very useful â€” mostly to see where we went wrong in our implementation.

## Complexity of Learning Algorithms

The reason for machine learning existing is because data sets are too large for
humans to analyze. The inputs that learning algorithms receive will be much
larger than most algorithms, so time complexity will be very important. For a
data set of ten-thousand, the difference between $O(n^2)$ and $O(n^3)$ is huge.

Learning algorithms can be roughly split into two components:

- _Learning_ (looking at training data); and
- _Application_ (applying the trained algorithm to data).

The complexity of a learning algorithm can be measured in both of these parts.

Given $n$ training inputs, the complexity of learning will be _at least_
$O(n)$; we have to at least look at each input once. Generally, this process is
expected to have more complexity and take longer than application.

Given $n$ inputs, we would like for application to be _no more_ complex than,
say, $O(n^2)$; any more and we risk taking too long to be useful.

## Supervised Learning

The book makes a point to introduce supervised learning here, but there is
really only one point to make.

In supervised learning, a sequence of points $(x_i, t_i)$ is given. For each
$x_i$, the element $t_i$ is the _correct_ answer to classifying $x_i$. The goal
is for the algorithm to learn from these pairs and be able to correctly match
each $x_i$ with $t_i$.

Usually, $(x_i)$ is a sample of the set of all possible inputs, so the
algorithm should also generalize its learning to inputs _not_ in $(x_i)$.

The regression example in 1.4.1 is a nice way to reframe what most people know
about regression. Given a sequence of points $(x_i, y_i)$, the sequence $(x_i)$
is the sequence of $x$-coordinates, and $(t_i)$ is the sequence of
$y$-coordinates.

The classification example in 1.4.2 is basically regression but with a discrete
range for the function.

## Examples of Noise

The _noise_ in a data set refers to the random fluctuations that are not
relevant for classification or prediction. As a fabricated example, a
collection of identical thermometers next to each other may give slightly
different readings due to the noise of the environment around them.

For a more concrete example, suppose that we are using supervised learning,
and we have the following two training points:
\begin{align*}
    ((0, 0, 0), 1) \\
    ((0, 0, 0), 0)
\end{align*}
After training, if the input $(0, 0, 0)$ is seen, there is no reliable way to
determine if the output should be $0$ or $1$. This is an extreme example of
noise, where one input appears to have two different targets associated with
it.

As a more realistic example, consider the following training points:
\begin{align*}
    ((0, 0, 0), 0) \\
    ((1, 1, 1), 1)
\end{align*}
If we then see the input $(0, 0, 1)$, it is possible that the single $1$ is due
to random noise. Hence it is difficult to decide if the output should be $0$ or
some intermediate value.
