# Preliminaries

The terminology introduced in 2.1 is very biased towards supervised learning,
and neural networks in particular. Input and output vectors ($\vec{x}$ and
$\vec{y}$) are pretty general; but weights and activation functions ($\vec{W}$
and $g(\cdot)$) are mostly associated with neural networks, and targets
($\vec{t}$) are only for supervised learning.

## Distance Formula

The distance formula is introduced here in the context of neural networks. The
rough idea is to treat weights of neurons as coordinates in space, and have the
neurons fire if they are "close enough" to an input vector. The distance
function can be used generally to measure how far away inputs are from some
target.

This concept doesn't seem to be used in the chapter, so it feels a little out
of place. 

## The Curse of Dimensionality

The point of the hypersphere argument is unclear. In summary, as the number of
dimensions increases, we will need much more data to train on. This is because
the data will, in general, tend to be more spread out.

To explain the hypersphere argument, construct a unit box in an arbitrary
number of dimensions. Think of the hypersphere as our target; that is, if we
pick a random point from the unit box, we want it to be in the sphere. If we
are picking the points randomly, the probability of a point being in the sphere
is $V(S) / V(B)$, where $V(S)$ and $V(B)$ are the volumes of the sphere and
box, respectively.

As the number of dimensions increases, the volume of the sphere does not grow
as quickly as the volume of the cube, so the probability of picking a point in
the sphere at random decreases simply because we added more dimensions.

This idea will apply to our learning algorithms. Our classifiers will be less
accurate as the number of dimensions in our inputs increases, just because of
the greater spread. Thus we will need much more data as the number of
dimensions we use increases.

## Accuracy Metrics

Accuracy metrics are useful to compare different methods of learning to each
other.

Leave out some, crossfold validation ($K$-Fold Cross Validation): Split data
into different chunks, and assign some chunks to be training, some to be
validation, and some to be testing. For every possible combination of
assignments, train a new model on the created data set, then take the best
model. (There was a very pretty picture.)

## Training, Testing, and Validation Sets

During learning, we are always given a data set of examples. In supervised
learning, we will partition the data into three different sets:

- _Training_: Used to train a classifier.
- _Validation_: Used to test the accuracy of a classifier independent of the
  the classifier has never been trained on, to test for overfitting.
- _Testing_: Used to test the final accuracy of a classifier against data it
  has never even seen.

It is important that these sets be disjoint. The point of the validation set is
to test both for accuracy and overfitting. If points from the validation set
are in the training set, then the classifier may overfit to the validation set,
destroying its intended metric. The same point can be made for the testing set.

It is also important that these sets be randomly chosen. If there is some sort
of bias, then the classifier could inherit this bias. For example, suppose that
a data set is sorted low-to-high for some feature. If we just take the first
60% of the data to be training, 20% to be validation, and 20% to be testing,
then the classifier would only be trainined on the lower-end values of this
feature.

Discussion questions:

- Why do we have training sets and test sets? (We need to test for
  generalization and overfitting.)

- What purpose does the validation set serve? (It offers an intermediate
  overfitting metric before testing.)

- When is the test set used? (Only after all training is complete.)

- Why is overfitting bad? (We might not be able to apply our algorithm to data
  outside of the training set.)

### $K$-fold Cross-Validation

When we have lots of data, partitioning the data into training, validation, and
testing sets is fairly simple. If we do not have lots of data, we can use the
method of $K$-fold cross validation.

Briefly:

- Randomly partition the data into $K$ subsets.
- Choose one subset for validation, another for testing, and combine the rest
  to be training.
- Train a classifier on these given sets.
- Repeat with a new classifier for all possible random partitions, then take
  the one with the best testing accuracy.

## Accuracy Metrics

Accuracy is simply the ratio of the number of correct classifications to the
total number of inputs: the estimated probability that we can correctly
classify inputs.

- _Sensitivity_ is the ratio correct positives over total positives present:
  the estimated probability that we can correctly classify inputs given that
  they are positive; cf. $P($Classified True $\mid$ True$)$. _Specificity_ is
  analogous, but for negatives; cf. $P($Classified False $\mid$ False$)$.

- _Precision_ is the ratio of correct positives over all classified positives:
  the estimated probability that an input was correctly classified positive
  given that we did classify it positive; cf. $P($True $\mid$ Classified
  True$)$. Sort of the converse of sensitivity^[This is bunk. -- Dr.
  Patterson].

- The _$F_1$ metric_ is the ratio of correct positives over the sum of correct
  positives and average number of misclassifications; it is one if and only if
  there are no misclassifications, and strictly less than one otherwise.

The text mentions the implicit assumption of a balanced data sets for these
metrics. This assumption comes into play because many of these metrics will be
misleading if a large majority of data is negative or positive. For example,
consider a data set that is a snapshot of infections in a population of a rare
disease. Most inputs will be negatives, so models that always classify negative
will have specificity of one, but will not have actually learned how to detect
the disease.

## The Receiver Operator Characteristic Curve

The ROC curve is mostly well explained. A curve that looks as smooth as the
ones in the book will require either a lot of points, or some kind of
regression curve.

## Na√Øve Bayes Classifier

Given the feature vector $\vec{X_j}$, to compute $P(C_i \mid \vec{X_j})$, we
need to compute $P(X_j^1 = a_1,\ X_j^2 = a_2,\ \dots,\ X_j^n = a_n \mid C_i)$.
We may not be able to find a point such that $X_j^k = a_k$ holds for every
valid $k$. This is an issue, because then the joint probability is impossible,
and so $P(\vec{X_j} = \vec{a} \mid C_i) = 0$.

To make this easier, we make the simplifying assumption that the components
$X_j^k = a_k$ are independent from each other. Then, $P(X_j^1 = a_1,\ X_j^2 =
a_2,\ \dots,\ X_j^n = a_n \mid C_i) = P(X_j^1 = a_1 \mid C_i) P(X_j^i = a_2
\mid C_i) \cdots P(X_j^n = a_n \mid a_n)$. This lets us find examples that
satisfy each value separately, instead of looking for examples that satisfy
_every_ value restriction.
