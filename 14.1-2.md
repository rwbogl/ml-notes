# Unsupervised Learning

Chapter 14 introduces us to the concept of _unsupervised learning_, or learning
where an algorithm must learn to classify data without being given
pre-classified examples.

The biggest change from supervised to unsupervised is the lack of targets to
train against. However, because there are no targets, we also can't use any of
our old error functions. They rely on targets or other external information
that we don't have on hand anymore. Thus, we need error functions that can
operate with only a set of datapoints.

As an example of how such an algorithm would work, the text introduces the
_$K$-means algorithm_.

## The $K$-Means Algorithm

The $K$-means algorithm is roughly the unsupervised analog of the $K$-nearest
neighbors algorithm. For $K$-nearest neighbors, an input was classified by
choosing the most common class out of the $K$-nearest examples. This idea
requires that we know the class of the examples; i.e., it is a supervised
algorithm. For $K$-means, we won't have those targets to pick from.

The $K$-means algorithm operates on the same assumption as $K$-nearest
neighbors: datapoints that are close to each other are likely in the same
class, and classes are usually far enough apart to be separated. The $K$-means
algorithm assumes that the data is split into $K$ classes which are separated
enough to be clustered together, then tries to find the center of each cluster
iteratively.

The $K$-means algorithm needs two things: a distance measure, or
[metric](https://en.wikipedia.org/wiki/Metric_(mathematics)), and a way to
compute the mean. Usually, we use the Euclidean metric, but there are others.

A solid outline of the $K$-means algorithm is given on page 283. One important
thing to note is that the learning process looks at every datapoint before
making any updates. This will change when we introduce neural networks to solve
the problem.

A final point is that the $K$-means algorithm doesn't know anything about class
labels. It can only cluster data, not say what that data is. For example,
suppose that we have a dataset of flowers with the classes "rose" and "lily."
We are given a set of flowers without labels, so we try $K$-means to cluster
the examples. This gives us two clusters, but we don't know which cluster
corresponds to roses, and which to lilies. Assuming that the first cluster
corresponds to the first class, say "rose," could be incorrect. There is not
much we can do to fix this problem.

### The $K$-Means Algorithm as a Neural Network

We can express an _on-line_ version of the $K$-means algorithm as a neural
network. On-line indicates that we will make updates to the network after
seeing each datapoint, not after seeing all of them. There are various reason
that we might want to do this. Perhaps we are being fed data one point at a
time and need to make predictions faster than we can receive data.

The neural network is a single-layer perceptron, with one input neuron for each
feature in the dataset. There are $K$ output neurons representing the $K$
clusters, whose weights are the locations of the center of each cluster. The
activation function used is something like the distance between two points
(more on that later). To choose which output fires, hard-max is used. That is,
the neuron that a point is closest to fires and gets updated. An outline of the
algorithm is given on page 289.

There are a few technical notes about the book's implementation: normalization
and the activation function. The activation function used is $g(\vec{x}) =
\vec{x} \cdot \vec{w}$, where $\vec{w}$ is the weight vector. As the book
claims, this _effectively_ computes the distance between $\vec{w}$ and
$\vec{x}$ under the assumption that $\vec{w}$ and $\vec{x}$ are unit vectors.
What they mean by this is that, if $\vec{x} \cdot \vec{w}$ is at a maximum,
then, $|\vec{w} - \vec{x}|$ is at a minimum. So we pick the output neuron that
maximizes $\vec{x} \cdot \vec{x}$ using hard-max, and we know that $\vec{x}$ is
closest to this neuron.

To see why this is true, we will examine $|\vec{w} - \vec{x}|^2$. When this is
minimized, then $|\vec{w} - \vec{x}|$ will also be minimized. Now,
\begin{align*}
    |\vec{w} - \vec{x}|^2 &= (\vec{w} - \vec{x}) \cdot (\vec{w} - \vec{x}) \\
                          &= \vec{w} \cdot \vec{w} - 2 \vec{x} \cdot \vec{w} +
                          \vec{x} \cdot \vec{x} \\
                          &= |\vec{w}|^2 - 2 \vec{x} \cdot \vec{w} +
                          |\vec{x}|^2.
\end{align*}
Since $\vec{x}$ and $\vec{w}$ are unit vectors, $|\vec{w}|^2 = |\vec{x}|^2 =
1$, so $$|\vec{w} - \vec{x}|^2 = 2 - 2 \vec{x} \cdot \vec{w},$$ or $$\vec{x}
\cdot \vec{w} = 1 - \frac{|\vec{w} - \vec{x}|^2}{2}.$$ Thus, when $\vec{x}
\cdot \vec{w}$ is maximized, so is $$1 - \frac{|\vec{w} - \vec{x}|^2}{2},$$
meaning that $$\frac{|\vec{w} - \vec{x}|^2}{2}$$ is at a minimum, and so
$|\vec{w} - \vec{x}|^2$ is as well.
