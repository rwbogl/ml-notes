# Markov Chain Monte Carlo (MCMC)

Previously, we considered ways to approximate sampling from a distribution that
is difficult to sample from. This included the Box-Muller Scheme (using a
uniform distribution to sample from a standard normal) and the rejection
sampling distribution (using an easier "proposal distribution" to approximate
samples). In this section, we look at using Markov chains to accomplish the
same thing.

## Markov Chains

A Markov Chain is a sequence of states where the probability of being in the
next state depends only on the current state. For examples, think of the
standard "random walk" or "Brownian motion" in probability.

The text does not make use of a lot of Markov chain theory, but it does place a
lot of technical restrictions on the chains. In particular, it requires that
the chains are strongly ergodic. This means that they are irreducible, positive
recurrent, and aperiodic. This doesn't ever make a big appearance in the
section, but it is quite easy to create a non-ergodic Markov chain. If
ergodicity is actually important, we should be careful when just playing with
Markov Chains.

## Markov Chains and Sampling

Some Markov chains have _limiting distributions_. These are analogous to stable
equilibrium points in differential equations. For a limiting distribution, no
matter what state we begin in, after a large number of steps, the probability
of our end position will be described by a constant distribution. Formally, if
$p^{(n)}_{ij}$ is the probability of being in state $i$ after $n$ steps from
state $j$, then $$\lim_{n \to \infty} p^{(n)}_{ij} = \pi_i,$$ where $\pi_i$ is
a constant. Because we require that our Markov chains be strongly ergodic, each
of them has a unique limiting distribution. (Take my word for it.)

Markov chains are very simple to sample from. Start at any state and take a
large number of random steps. The end state is our sampled value. If we can
create an ergodic Markov chain whose limiting distribution is the distribution
we want to sample from, then we can approximate samples from that distribution.
For example, if our Markov chain had a Poisson limiting distribution, then the
end state after a large number of random walks would approximately be a random
value from a Poisson distribution^[It will not do to simply select a state at
random. This would be sampling from a uniform distribution. The walks have to
actually be performed, since that's what the limiting distribution requires.].

To generalize the idea of sampling, we have a proposal distribution $q(x_i \mid
x_i)$. That is, a distribution whose next sample depends only on the current
sample. This is sort of a "generalized" Markov chain. We start with an initial
guess and then we take another sample from this proposal distribution. We take
this new sample only if it is roughly "more likely" than the current sample.
This is done by computing an "acceptance ratio" and accepting the point with
probability equal to this acceptance ratio.

The algorithm runs as follows:

__Metropolis-Hastings Algorithm:__

1. Decide to sample $n$ values and pick an initial sample $x_0$.

2. Set $k = 0$.

3. Loop until $k = n$:

    i. Sample a new value $x^*$ from the proposal distribution.

    ii. Compute the acceptance ratio $$\alpha = \min \left(1, \frac{p(x^*)
    q(x^* \mid x_k)}{p(x_k) q(x_k \mid x^*)} \right).$$

        - Note that $\alpha$ increases if $q$ says that moving to $x^*$ from
          $x_k$ is more likely than moving the opposite direction the current
          sample. It also increases if $p$ says that $x^*$ is more likely than
          $x_k$ in general.

    iii. Pick a uniform number $u$ from $[0, 1]$.

    iv. If $u < \alpha$, then set $x_{k + 1} = x^*$ and $k \leftarrow k + 1$.
    Otherwise, repeat.

4. Celebrate with your $n$ samples.
