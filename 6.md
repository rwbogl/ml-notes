# Dimensionality Reduction

The book makes a big deal about the "curse of dimensionality," referring to how
computational complexity and the amount of data needed to train a learning
algorithm increases with the number of dimensions in a dataset. In this
chapter, we will learn ways to fight the curse.

There are three main ways to do this:

- __Feature selection__: Throw out features that aren't useful to learning. We
  saw an example of this with tree learning. We looked at every feature and
  decided what would be the best one to examine, adding features until we had a
  perfect classifier or had no more features.

- __Feature derivation__: Create new features using combinations of old ones,
  then throw out the old ones. See Figure 6.1 for an example.

- __Clustering__: Group together similar datapoints and see if this provides
  hints for features that could be removed.

## Linear Discriminant Analysis (LDA)

Linear discriminant analysis attempts to cluster data by projecting it onto a
hyperplane. The hope is that classes will be separated far enough on the
hyperplane so that their distance along the line will be enough to cluster them.
This reduces the dimensions of the data down to one, a single scalar.

There are two metrics that we consider in LDA:

- __Within-class scatter__: The total amount of scatter between datapoints
  inside the same class. The total "spread" of the dataset on an _intra_-class
  level. Measured as $$S_W = \sum_{\text{classes} \ c} p_c \operatorname{cov}(c,
  c),$$ where $p_c$ is the probability of a class occurring, and
  $\operatorname{cov}(c, c)$ is the covariance matrix of the class $c$. Note
  that this is a matrix.

- __Between-class scatter__: The total amount of spread between classes
  themselves. The total "spread" of the dataset on an _inter_-class level.
  Measured as $$S_B = \sum_{\text{classes} \ c} (\vec{\mu}_c -
  \vec{\mu})(\vec{\mu}_c - \vec{\mu})^T,$$ where $\vec{\mu}$ is the mean of the
  entire dataset. Note that this is a matrix, as we are dealing with column
  vectors.

We would like for $S_W$ to be small, and $S_B$ to be big. That is, we would like
to maximize the ratio $S_B / S_W$.

We will be maximizing this ratio by choosing the hyperplane with the "best"
spread to project our data onto, described by the unit vector $\vec{w}$. For any
datapoint $\vec{x}$, the scalar projection onto our hyperplane is $\vec{w} \cdot
\vec{x}$.  If we compute the within-class and between-class scatter using the
scalar projections, we obtain (through some linear algebra)

\begin{align*}
    S_W' &= \vec{w}^T S_W \vec{w} \\
    S_B' &= \vec{w}^T S_B \vec{w}.
\end{align*}

Thus, we want to maximize the ratio $\frac{\vec{w}^T S_W \vec{w}}{\vec{W^T} S_B
\vec{w}}$. In general, this is only easy if we have two classes. With two
classes, the plane that will maximize this ratio is in the direction of
$$\vec{w} = S_W^{-1} (\mu_1 - \mu_2).$$

## Principal Components Analysis (PCA)

Principal Components Analysis attempts to separate unlabeled data by
transforming points so that only dimensions that datapoints "vary" along are
examined. It is a type of feature selection.

\newcommand{\cov}[1]{\operatorname{cov}\left( #1 \right)}

The motivation for the algorithm is very linear algebra heavy. The gist is this:

- We have a (centered) data matrix $X$ with covariance matrix
  $\operatorname{cov}(X)$.
- Besides centering the data, we now want to rotate it. This will be done by
  multiplying by a rotation matrix $P^T$. Our new dataset is $Y = P^T X$.
- We want, for some reason, to choose $P^T$ such that $\operatorname{cov}(Y)$ is
  diagonalizable. That is, $$\cov{Y} =
  \begin{bmatrix}
    \lambda_1 & \cdots & \cdots & \cdots \\
    \vdots & \lambda_2 & \cdots & \cdots \\
    \vdots & \vdots & \lambda_3 & \cdots \\
    \vdots & \vdots & \vdots & \vdots \\
    \cdots & & & & \lambda_n
  \end{bmatrix}.$$
- From definition of covariance, we arrive at $\operatorname{cov}(Y) = P^T
  \operatorname{cov}(X)P$. Since $P$ is a rotation matrix, its transpose is its
  inverse. This leads us to $$P \operatorname{cov}(Y) = P P^T \cov{X} P =
  \cov{X} P.$$
- If we write $P$ as a list of column vectors $P = [\vec{p}_1, \vec{p}_2, \dots,
  \vec{p}_n]$, then we have $$P\cov{Y} = [\lambda_1 \vec{p}_1, \lambda_2
  \vec{p}_2, \dots, \lambda_n \vec{p}_n] = \cov{X}P.$$
- Since the two matrices are equal, so are their columns. Splitting this into a
  system of equations, we have $$\vec{\lambda} \vec{p}_k = \cov{X} \vec{p}_k.$$
- This leads us to the conclusion that the columns of $P$ are eigenvectors of
  $\cov{X}$.
- The covariance is square and symmetric, so its full complement of eigenvalues
  are orthogonal, and thus form an eigenspace. This means that each eigenvalue
  roughly corresponds to one dimensions. We want to choose the dimensions that
  have the larger eigenvalues.

Algorithmic sketch:

1. Begin with centered data matrix $X$.
2. Compute the covariance matrix $C$.
3. Find the eigensystem of $C$, and arrange the eigenvectors in decreasing
order.
4. Select a number of dimensions to keep, then take this number of the largest
eigenvectors.
5. Use the selected eigenvectors to transform the data, dropping the dimensions
that aren't selected.


## Factor Analysis

The text's explanation of factor analysis is esepcially brief.

Factor analysis works to create a simple linear model to describe a dataset
where the elements of the model are noisy random variables. That is, given a
data matrix $X$, we construct the model $$X = W F + \epsilon,$$ where $F$ is a
random matrix describing a set of "factors" that we think are largely
responsible for the data, $W$ is a matrix of "factor loadings" that describe
how the factors affect the measurements, and $\epsilon$ is a normal random
matrix with mean zero and variances $\psi_i$ that represents the "noise" in the
data. Our goal is to find a matrix $W$ and set of variances $\psi_i$ that
maximize the likelihood of observing $X$ given our model.

As a brief example, say that $X$ are the results of an IQ test. Then, we might
have $$F =
\begin{bmatrix}
    \text{IQ}_1 & \text{Height}_1 \\
    \text{IQ}_2 & \text{Height}_2
\end{bmatrix}$$
and $$W =
\begin{bmatrix}
    1 & 0 \\
    1 & 0
\end{bmatrix},$$
since the results of the IQ test are dependent only on the subject's IQ.

Factor analysis is an EM algorithm, meaning it is composed of two very simple
steps that are impossible to explain. Essentially, we compute the expectation
of observing our data given the current model, differentiate it to maximize the
expectation, update $W$ and each $\psi_i$, and repeat until convergence. The
steps are outlined on pages 141 and 142, with a partial code listing on page
142.

## Locally Linear Embedding

(The text has a lot of confusing notation in this section. I decdided what
things should be, but the code listings given should probably be consulted.)

Locally Linear Embedding (LLE) works to reduce the dimensionality of a dataset
by transforming higher dimensional vectors into lower dimensional ones that
minimize the "reconstruction error" of the lower dimenison. Broadly speaking,
the algorithm chooses neighborhoods around points, then places the points into
a lower dimensional space around the neighborhood that minimizes some error
metric.

The algorithm first chooses a neighborhood around each datapoint. These
neighborhoods can be created in many different ways, e.g. in the "topological"
sense of all points within a distance $d$, or in the $k$-nearest neighbors
sense of grabbing the closest $k$ points.

After choosing a neighborhood around a point $\vec{x}_i$, the remaining points
are used to "reconstruct" (approximate) $\vec{x}_i$. Each point in the dataset
is assigned a weight in this reconstruction, denoted $W_{ij}$. For every
$\vec{x}_j$ not in the neighborhood of $\vec{x}_i$, we set $W_{ij} = 0$. Then,
we have $$\vec{x}_i \approx \sum_{j = 1}^N W_{ij} \vec{x}_j,$$ where $N$ is the
number of datapoints. The dataset is then assigned a reconstruction error
$\epsilon$ that is the sum-of-squares error in the reconstruction
approximations: $$\epsilon = \sum_{i = 1}^N \left| \vec{x}_i - \sum_{j = 1}^N
W_{ij} \vec{x}_j \right|^2.$$

There is no need to construct $W$ by hand. There is a method for choosing $W$
such that $\epsilon$ is minimized:

- For each point $\vec{x}_i$, construct its neighborhood. Create a list of
  neighbor points $\vec{z}_j$, $j = 1, 2, \dots, K$. Let $\vec{d}_j = \vec{z} -
  \vec{x}_i$ be the difference from each neighbor to $\vec{x}_i$. Form a matrix
  $D$ of these differences. ($D$ must be $N \times N$, which I am not sure how
  neighborhoods are guaranteed to be disjoint. For the brave, there is a
  partial code listing on page 146.)

- Compute the "local covariance," defined as $C = D D^T$. (I believe that this
  works out to actually be the covariance of $D$, but it's hard to follow with
  all the vectors around.)

- Solve $CW = I$ for $W$, where $I$ is the $N \times N$ identity. That is, $W =
  C^{-1}$.

- Set $W_{ij} = 0$ when $x_j$ is not in the neighborhood of $x_i$. (In the
  topological sense, I feel confident that "in the neighborhood of" is a
  symmetric relation, but not so sure for $K$-nearest neighbors.)

- Normalize the elements so that the matrix sums to one, i.e. set $W_{ik} =
  W_{ij}/\sum W_{ij}$.

This gives us an "ideal" reconstruction matrix $W$ to play with.

Next, we get to the "embedding" part. We will choose an arbitrary lower
dimension $L$, and construct the point $\vec{y}_i$ that corresponds to
$\vec{x}_i$ in $\mathbf{R^L}$. In this lower dimensional space, we create a new
reconstruction error $\epsilon_L$, defined as $$\epsilon_L = \sum_{i = 1}^N
\left| \vec{y}_i - \sum_{j = 1}^L W_{ij} \vec{y}_j \right|^2.$$ Note the
similarity with the previous reconstruction error. To futher this analogy, we
want to choose $\vec{y}_i$ such that $\epsilon_L$ is minimized. The text punts
hard, and says that these vectors are given by the eigenvectors of the
_quadratic form matrix_ $$M = (I - W)^T (I - W).$$

The full algorithm strings these steps together, and can be found on page 145.
