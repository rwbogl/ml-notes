# MLP in Practice

Recall that slicing on numpy arrays can be done in multiple dimensions. For
example, let `x` be a numpy matrix. Then:

- `x[0]` returns the first row.
- `x[0:3, 0]` returns the first element of the first three rows.
- `x[:3, 0]` is the same as above.
-  `x[:3, :6]` returns the first six elements of the first three rows.
- `x[:3, -3:]` returns the last three elements of the first three rows.

## When to Stop?

The naïve method is to train for $T$ batches, for some constant $T$. There is
no theoretical basis for this. As Dr. Patterson puts it, this method "kinda
sucks."

The next, slightly less naïve method is to set an accuracy threshold for the
training set. This could end in an infinite loop if that accuracy is never
reached.

The more analytic approach is to make use of the validation set. In theory,
while the MLP is learning the dataset or function, validation error will be
decreasing. As we begin to overfit the data or function, the validation error
will begin to increase again. Thus we will try to stop training at a local
minimum of the validation error.

Finding this local minimum can be tricky. The book suggests keeping three
previous models during training. If either of the two most recent validation
errors has decreased enough from the validation error before themselves, then
continue training. We check both so that a minor fluctuation in validation
error will not halt training prematurely.

## Probably Approximately Correct Learning

The book says that, given $W$ weights, we should have roughly $10W$ datapoints
to train on. Dr. Patterson says that, given $L$ inputs, we should have roughly
$2^L$ datapoints.

Neither of these have much theoretical grounding -- they are just practical
rules of thumb. The subset of machine learning called _Probably Approximately
Correct Learning_ is more theoretically grounded.

## Universal Approximation Theorem

The Universal Approximation Theorem states that any neural network, with any
number of of hidden layers and nodes, can be approximated by a single hidden
layer MLP with some amount of nodes. This amount may be very large.

(Actually, it's about approximating convex functions. See [the formal
statement](https://en.wikipedia.org/wiki/Universal_approximation_theorem).)

## 1-of-$N$

Instead of using one number to encode $N$ classes, 1-of-$N$ classifiers use an
$N$-tuple with components zero and one. As an example, with three classes,
class $0$ would become $(1, 0, 0)$, class $1$ $(0, 1, 0)$, and class $2$ $(0,
0, 1)$.

## Compression

Neural network compression is very lossy. By cutting down from $k$ inputs to $N
< k$ activations in the hidden layer, we will always lose information.

(Can we get a bound on this? For example, if the hidden layers are activating
to some finite set of integers, say zero and one, then there would be $2^L$
possible inputs to remember.)

## Recipe for MLP

1. Select inputs and outputs.

2. Normalize inputs to some interval, usually $[0, 1]$.

3. Split dataset into training, validation, and testing.
    - Various splits are possible. Common ones are 50/25/25 and 60/20/20.

4. Decide on NN architecture.
    - How many layers?
    - How many nodes in each layer? (Too many and we lose accuracy, not enough
      and we might overfit.)

5. Train network until validation
