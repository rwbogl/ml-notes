# Learning with Trees

\newcommand{\entropy}[1]{\operatorname{Entropy} #1}
\newcommand{\gain}[2]{\operatorname{Gain}\left( #1, #2 \right)}

This section corresponds to Chapter 12. Some of the code in this chapter lacks
a frusterating amount of documentation.

- ID3 is greedy; it grabs the feature with the greatest gain at each step. When
  there are no features left, it grabs the most common class and hopes that it
  works.

    - It's worth noting that the book requires computing the entropy of the
      entire dataset. We don't have to! Gain is given in Equation 12.2. When
      trying to maximize this, $\entropy(S)$ is constant, so the feature that
      has the smallest $- \sum_{f \in values(F)}$ term will maximize gain.

        To see this, let $\sum_F$ and $\sum_{F'}$ be the sigma terms for two
        different features. If $$\sum_F < \sum_{F'},$$ then applying the
        decreasing function $f(x) = \entropy{S} - x$ to this inequality gives
        $$\entropy{S} - \sum_F > \entropy{S} - \sum_{F'},$$ or $$\gain{S}{F} >
        \gain{S}{F'}.$$

- The entropy of a set with classes $C_1,\ C_2,\ \dots,\ C_n$ is calculated
  with the probability $p_k$ being the probability of the class $C_k$ occurring
  in the data set. Equation 12.3 shows this with two classes: true and false.

- `calc_entropy` does not actually compute the entropy that these sections talk
  about; it only _helps_ compute it.

- `calc_info_gain` does not actually compute the gain; it computes the sigma
  term from Equation 12.2, and `make_tree` later computes the gain. It also
  generalizes to an arbitrary number of classes without telling us. Here's my
  best guess at documentation:

```python
def calc_info_gain(data, classes, feature):
    """Compute the sigma term from Equation 12.2 from choosing the given feature.

    data: List of vectors [feature_1, feature_2, ..., feature_n].
    feature: Integer that indexes the feature to be used from the dataset.
    classes: List of the same length of `data`, where `classes[k]` is the class
             of the kth datapoint.

    The possible values will be found by looking directly at the data.
    """
```

## ID3 Example Setup

Here is the setup for an ID3 example (this was the majority of a class period):

Let sex denote our class, with Male and Female being values. We have the
attributes Height (Tall, Medium, Short) and Weight (Heavy, Median, Light).

Consider the following dataset:

Height  | Weight  | Sex 
--|---|--
T | H | M
S | L | F
M | M | M
M | M | M
S | H | M
S | L | F
T | L | F
T | L | M
M | L | F

From the note above, calculate the sigma term in Equation 12.2 for each Height
and Weight, then split on the feature that has the smallest.
