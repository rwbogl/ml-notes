# Optimization and Search

## Line Searches and Gradient Descent

\newcommand{\reals}{\mathbf{R}}

In many types of learning, we want to minimize some error function $f$. There
are many ways to do this, but the simplest is called a _line search_. In this
method, we have some initial guess for the minima, $\vec{x}_0$, and we follow a
line in some preset direction for some preset distance to the next guess. That
is, we pick our next guess with the equation $$\vec{x}_{n + 1} = \vec{x}_n +
\eta_n \vec{p}_n,$$ where $\vec{p}_n$ is the direction of the line, and
$\eta_n$ is the distance to follow along the line.

With this equation, we are free to pick the direction $\vec{p}_n$ and the
distance $\eta_n$. Different methods of choosing these will give us different
search techniques. For example, setting $\vec{p}_n = -\nabla f(\vec{x}_n)$
gives us gradient descent.

The choice of $\vec{p}_n = -\nabla f(\vec{x}_n)$ is usually due to the fact
that $-\nabla f(\vec{x}_n)$ is the direction of greatest decrease for $f$.
However, we can also derive this direction by considering different
approximations for $f$.

These approximations are extensions of taylor expansions of single-variable
functions. Recall that a single variable function $g(x)$ may be approximated as
$$g(x) \approx g(x_0) + g'(x_0)(x - x_0) + g''(x_0)(x - x_0)^2 + \cdots.$$ For
functions of multiple variables, we will replace $g'$ and $g''$ with more
general "derivatives" called the gradient and Hessian, respectively.

First, we have the linear approximation $$f(\vec{x} + \vec{p}) \approx
f(\vec{x}_n) + \nabla f(\vec{x}_n) \cdot \vec{p} + o(|\vec{p}|).$$ If we
minimize this with respect to $\vec{p}$, we end up with $\vec{p} = -\nabla
f(\vec{x}_n)$, which recovers gradient descent.

Next, we have the quadratic approximation $$f(\vec{x} + \vec{p}) \approx
f(\vec{x}_n) + \nabla f(\vec{x}_n) \cdot \vec{p} + \frac{1}{2} \vec{p}^T
H(f(\vec{x_n})) \vec{p} + o(|\vec{p}|^2),$$ where $H(f(\vec{x}))$ is the
Hessian matrix of second derivatives, $$H(f(\vec{x})) =
\begin{bmatrix}
    f_{x_1 x_1}(\vec{x}) & f_{x_1 x_2}(\vec{x}) & \cdots & f_{x_1 x_n}(\vec{x}) \\
    f_{x_2 x_1}(\vec{x}) & f_{x_2 x_2}(\vec{x}) & \cdots & f_{x_2 x_n}(\vec{x}) \\
    \vdots & \vdots & \vdots & \vdots \\
    f_{x_n x_1}(\vec{x}) & f_{x_n x_2}(\vec{x}) & \cdots & f_{x_n x_n}(\vec{x})
\end{bmatrix}.$$
If we minimize this equation with respect to $\vec{p}$, then we obtain
$$\vec{p} = - (H(f(\vec{x}_n)))^{-1} \nabla f(\vec{x}_n).$$ This direction is
called the _Newton direction_.

The computational complexity of the Newton direction is fairly high; we need to
compute the inverse of the Hessian matrix. However, according to the text, the
payoff is that our step size $\eta$ is always set to one.

## Conjugate Gradients

The goal of conjugate gradients is to spend a little more time thinking about
what directions and how far along them to step to minimize the number of steps
taken. In fact, the goal is, in $n$ dimensions, to take exactly $n$ steps to
reach the minimum of a function.

Except for when the error function is linear, this goal is almost never
reached, but the method of conjugate gradients will still improve on line
searches.

For the direction $\vec{p}_n$, conjugate gradients makes two choices. For the
first step, we follow gradient descent and set $\vec{p}_0 = -\nabla
f(\vec{x}_0)$. After this, we apply what is called a _Gram--Schmidt process_ to
discover the directions. In $n$ dimensions, we create the sequence of $n$
coordinate axis vectors $\vec{u}_k$. Then, the direction is given by
$$\vec{p}_n = \vec{u}_n + \sum_{k = 1}^{n - 1} \beta_{k} \vec{p}_k,$$ where
$$\beta_{k} = \frac{|\nabla f(\vec{x}_k)|^2}{|\nabla f(\vec{x}_{k - 1})|^2}.$$

For the step size $\alpha_n$, conjugate gradients uses Newton--Raphson
iteration to derive the optimal $$\alpha_n = \frac{\nabla f(\vec{x}_n) \cdot
\vec{p}}{\vec{p}^T H(f(\vec{x}_n)) \vec{p}}.$$

From here, the algorithm on page 200 is fairly straightforward. It is
essentially the line search, but making the choices described above.
