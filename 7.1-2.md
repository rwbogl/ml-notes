# Probabilistic Learning

A criticism of neural networks is that we cannot "see" what they are doing.
Weights and biases may work, but they have no intuitive meaning. Other learning
methods work well and are more transparent. Decision trees^[Decision trees,
Chapter 12, are referred to in the past tense in this chapter, Chapter 7.] are
one example. In this chapter, we will present another, based on statistical
methods.

Highlights:

- EM Algorithm
- Unsupervised learning example
- Nearest neighbor methods
- Statistical methods

## Gaussian Mixture Models

Note that the text says, in Chapter 7, that "we _will_ [emphasis added] see
lots of ways to deal" with unlabeled examples in Chapter 6. We have already
seen Chapter 6, in theory.

- $M$: number of Gaussian distributions.
- $\alpha_m$: "weight" of distribution $m$.
- $\sum_{m = 1}^M \alpha_m = 1$.
- $\phi(x; \mu_m, \sigma_m)$: normal function with specified parameters.

Define $$P(x_i \in C_k) = \frac{\alpha_k \phi(x_i; \mu_k, \sigma_k)}{\sum_{m =
1}^M \alpha_m \phi(x_i; \mu_m, \sigma_m)}$$ to be the probability of input $x_i$
belonging to class $C_k$.

Say that we have two normals $G_1$ and $G_2$, where $p$ is the probability of a
datapoint beloning to $G_1$. Suppose that $p$ is also a random variable, and
has probability density function $\pi$. From the above model, we have the
random variable $$y = pG_1 + (1 - p)G_2.$$ The probability of any particular
value of $y$ occuring is $$P(y) = \pi \phi(y; \mu_1, \sigma_1) + (1 - \pi)
\phi(y; \mu_2, \sigma_2).$$

Our goal now is to iterate, trying to move the Gaussian models to fit our data
correctly. For example, we will move $\mu_1$ until the curve over $\mu_1$ seems
to be a good fit of the data.

Essentially, $\gamma_i$ is an estimate of $P(x_i \in C_1)$. After calculating
every $\gamma_i$, we have a rough picture of what the data looks like from our
previous guesses. From this, we move the means, standard deviations, and
probability $\pi$ around so that we fit the data better. We repeat this until
our parameters converge.

Example:

$$D = \{5, 10, 4, 6, 5, 4, 6, 11, 10, 9, 5, 7, 3\}.$$
From drawing a graph, we conjecture that there are two normals, with
\begin{align*}
    \mu_1 &= 5, \quad \sigma_1 = 2, \quad \pi = \frac{9}{13} \\
    \mu_2 &= 10, \quad \sigma_2 = 1.
\end{align*}

After performing first M-step, we have

\begin{align*}
    \mu_1 &= 4 \\
    \mu_2 &= 11 \\
    \bar{y} &= 6.54 \\
    \sigma_1 = \sigma_2 &= 6.38 \\
    \pi = 0.4
\end{align*}

## Information Criteria

We are familiar with the validation set. This set can be used to determine when
to stop training, i.e. stop when the validation error is at a local minimum.
However, there are other ways to determine when to stop training. In
particular, one way to do this is by measuring _information criteria_.

Given models, we have two information criteria. The _Akaike Information
Criterium_ $$\text{AIC} = \ln \mathcal{L} - k,$$ and the _Bayesian Information
Criterium_ $$\text{BIC} = 2\ln \mathcal{L} - k \ln N.$$ Here, $k$ is the number
of parameters in the model, $N$ is the number of datapoints, and $\mathcal{L}$
is "best likelihood of the model." The book is very quiet about what this is or
how to calculate it.

## EM Algorithm

1. __Expectation__: Compute the expected liklihood of the model.

2. __Maximization__: Update the model to maximize the expected likihood.
