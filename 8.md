# Support Vector Machines

## Perceptrons and their Limitations

In Chapter 3, we encountered a simple type of neural network which could
compute many functions: the __Perceptron__.

As a quick summary of the material in the Perceptron section, recall:

- The perceptron defines a _decision boundary_, $\vec{x}\vec{W}^T = 0$

    - In 2D, this is a line between classes.
    - In 3D, this is a plane.
    - In higher dimensions, we call this a hyperplane.

- The Perceptron Convergence Theorem guarantees finite-time convergence for
  linearly separable data sets.

    - If we can draw a line, or plane, or hyperplane between classes in input
      space, then the perceptron will find a line that can do this.

    - This will happen in a finite amount of time.

Now, this theorem only applies to linearly separable datasets. If a dataset is
not linearly separable, then there might be issues.

- Perceptrons might not work if the input space is not linearly separable.

    - For example, take the 2D XOR function.

    - Cannot draw a line between the XOR data, so it's not linearly separable.

    - A perceptron will waffle between incorrect inputs, and never converge to
      a correct answer.

    - This can be fixed by adding extra dimensions.

## Enter: Support Vector Machines

(This was written for section 8.1, but also from a [Stanford webpage on
SVMs](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html).)

Support Vector Machines (SVMs) provide two important things:

1. An objective way to choose the _best_ decision boundary; and

2. A set of methods to transform any dataset into one that is linearly
separable. (Example: 3D XOR function.)

That is, using SVMs, we can always linearly separate data by transforming it.

Also, after a brief training period, SVMs only need to remember a select few
training points to classify new points.

- The _margin_ of a linear separator is the largest radius $M$ such that no
  data points lie within $M$ units of the hyperplane defined by the separator.

- If there exists a linear separator with a margin larger than any other, it is
  the _maximum margin classifier_. This is considered to be the optimal linear
  separator.

- Points that are exactly $M$ units away from a linear classifier are called
  _support vectors_. (Measured by $|\vec{w} \cdot \vec{x} + b| = M$.)

- The margin of a linear classifier with weights $\vec{w}$ is $1/|\vec{w}|$.
  (The derivation is rather long, and very confusing in our textbook. See the
  Stanford webpage for a better derivation.)

- Given a point $\vec{x}$ with target $t = \pm 1$, a measure of the goodness of
  the classification of $\vec{x}$ is the _functional distance_ $t(\vec{w} \cdot
  \vec{x} + b)$.

    - General measure of how well classified a point is; the larger the value
      is, the better it was classified.

    - Positive for correct classifications, and negative for incorrect
      classifications.

### Finding the Maximum Margin Classifier

We need to

1. Find the largest margin possible; and

2. Make sure that classifications are still "good."

Maximizing the margin is the same as minimizing $\frac{1}{2} |\vec{w}|^2$.  The
$\frac{1}{2}$ factor is for convenience, and the square does not effect
anything, since the square root function is increasing for nonnegative inputs
anyway.

For the "good" requirement, we arbitrarily require that the functional distance
is greater than one. That is, that $t (\vec{w} \cdot \vec{x} + b) \geq 1$.
Equality is obtained only with support vectors.

This gives us a constrained optimization problem:

Let $\vec{x_i}$ and $t_i$, $i = 1, 2, \dots, n$ be sequences of input vectors
and targets, respectively, where $t_i = \pm 1$. Then,

$$\text{minimize } \frac{1}{2} |\vec{w}|^2 \text{ under the constraint } t
(\vec{w} \cdot \vec{x} + b) \geq 1, i = 1, 2, \dots, n.$$

This problem can be solved using the Karush–Kuhn–Tucker (KKT) method, which is
a generalization of the method of Lagrange multipliers that allows for
inequalities. Once the problem has been set up, we can place it into a solver
that will do the heavy lifting for us. We will not get too much into this right
now^[Because I don't understand it.], but here are the high points:

- We seek a $\vec{w}^*$ and $b^*$ such that $\vec{w}^* \cdot \vec{x} + b* = 0$
  is a maximum margin classifier.

- The solver will provide us with a vector of Lagrange multipliers
  $\vec{\lambda}$ such that $$\vec{w}^* = \sum_{k = 1}^n \lambda_k t_k
  \vec{x_k}$$ and $$b^* = \frac{1}{N} \sum_{\text{support vectors } s}
  \left(t_j - \sum_{k = 1}^n \lambda_k t_k \vec{x_k} \cdot \vec{x_s} \right),$$
  where $N$ is the number of support vectors.

This method will allow us to find an optimal minimum classifier for any
linearly separable dataset. The next step for SVMs is handling datasets that
are not linearly separable.

### Questions and Clarifications

- In the definition of a margin, $M$ is defined to be the largest radius such
  that no datapoints lie within $M$ units of the hyperplane. In Figure 8.2, the
  margin bounding region is drawn with a solid line, indicating that no points
  lie exactly $M$ units away. In other words, for every datapoint $\vec{x}$, we
  have $|\vec{w} \cdot \vec{x} + b| > M$.

    Almost immediately, support vectors are defined to be the points $\vec{x}$
    such that $\vec{w} \cdot \vec{x} + b = M$. So, let's all just agree that the
    boundary line in Figure 8.2 should be dotted, and that every datapoint
    $\vec{x}$ satisfies $|\vec{w} \cdot \vec{x} + b| \geq M$.

- After Equation 8.11, the text mentions that the SVM's optimal linear
  classifier has the property that prediction relies on computing the dot
  product of the input vector and _only_ the support vectors. However, the
  relevant term is $$\vec{z} \cdot \sum_{k = 1}^n \lambda_k t_k \vec{x_k},$$
  which is certainly over all training vectors $\vec{x_k}$, and not just the
  support vectors. What are they talking about?

## Kernels

As previously mentioned, the point of a linear classifier to draw a hyperplane
between linearly separable datasets. This was not always possible when datasets
were not linearly separable, as in the case of the XOR function. To fix this,
we will introduce kernels, which are computationally simple ways to make any
dataset linearly separable.

### Transformations

Let $X$ be our input space; usually this is some $\mathbf{R^n}$. To transform
our datapoints $\vec{x} \in X$, we will define a _feature mapping_ $\phi\colon
X \to V$ such that $\phi(\vec{x})$ is the transformation of the input
$\vec{x}$, where $V$ is some other input space, usually of a higher dimension.

Assuming that this mapping creates a linear separable dataset, we can apply the
KKT method to find our optimal linear classifier in this new input space,
giving us the optimal parameters

\begin{align*}
\vec{w}^* &= \sum_{k = 1}^n \lambda_k t_k \phi(\vec{x_k}) \\
b^* &= \frac{1}{N} \sum_{\text{support vectors } s}
        \left(t_j - \sum_{k = 1}^n \lambda_k t_k \phi(\vec{x_k}) \cdot \phi(\vec{x_s}) \right)
\end{align*}

To make predictions of a new input $\vec{z}$, we need to map $z$ with $\phi$,
then compute $$\vec{w}^* \cdot \phi(\vec{z}) + b^*,$$ using these vectors from
the new input space $V$. The vectors of $V$ may have a very large dimension,
and this may take a very long time. To get around this computation, we will
introduce _kernels_.

### Kernels and the Kernel Trick

Let $K\colon X^2 \to \mathbf{R}$ be a symmetric mapping. Then, $K$ is a _kernel
function_ iff it is positive definite (p.d.), as defined below. That is, the
function $K$ is a kernel iff

- (Positive definite) For all $<c_0, c_1, \dots, c_n> \in \mathbf{R^n}$ and
  $\vec{x_i} \vec{x_j} \in X$, $$\sum_{\substack{1 \leq j \leq n \\ 1 \leq i
  \leq n}} c_i c_j K(\vec{x_i}, \vec{x_j}) \geq 0;$$
- (Symmetric) $K(\vec{x}, \vec{y}) = K(\vec{y}, \vec{x})$ for all $\vec{x},
  \vec{y} \in X$.

The key point of this definition is an application of Mercer's
Theorem[^mercer]: _if there exists a positive definite kernel on an input space
$X$, then there exists a feature mapping $\phi\colon X \to V$ such that
$K(\vec{x}, \vec{y}) = \phi(\vec{x}) \cdot \phi(\vec{y})$._ That is, if we have
a kernel, then we are always implicitly computing the dot product of vectors in
a higher dimension, without ever working in that higher dimension. Using
kernels to avoid this higher-dimensional computation is the _kernel trick_.

[^mercer]: Mercer's Theorem is a result in functional analysis. We will not
talk about it or functional analysis ever again.

Practically, the kernel trick means that by replacing all dot products that the
SVM computes with the kernel computation, we will be implicitly using higher
dimensions. To force linear separability, we punt into a higher dimension, then
the kernel trick punts us right back to where we started.

With the kernel trick in mind, we do not need to find a feature mapping $\phi$
anymore; we only need to find positive definite kernels that are easy to
compute. Luckily, there are a few standard kernels and functions that they come
from:

- The polynomial kernel of degree $d$: $K(\vec{x}, \vec{y}) = (1 + \vec{x}
  \cdot \vec{y})^d$.

- The sigmoid kernel with parameters $k$ and $\delta$: $K(\vec{x}, \vec{y}) =
  \tanh(k \vec{x} \cdot \vec{y} - \delta)$.

- The radial basis function with parameter $\sigma$: $K(\vec{x}, \vec{y}) =
  \exp(-|\vec{x} - \vec{y}|^2 / 2\sigma)$. (The text incorrectly gives the
  vector factor as $(\vec{x} - \vec{y})^2$, which is not a scalar.)

### Kernel Clarification

Question: How does using a kernel actually avoid the computation in the higher
dimension? The formally doesn't readily explain this. Answer: Because of vector
stuff.

Recall that the optimized weight vector, $\vec{w}^*$, is given by the equation
$$\vec{w}^* = \sum_{k = 1}^n \lambda_k t_k \vec{x_k},$$ where $t_k$ is the
target for $\vec{x_k}$, and $\vec{\lambda}$ is given by `cvxopt` after
employing KKT. If we use a mapping $\phi\colon X \to V$, then the
classification computation for an input $\vec{z}$ becomes $$\left(\sum_{k =
1}^n \lambda_k t_k \phi(\vec{x_k}) \right) \cdot \phi(\vec{z}) + b^*.$$ Of
course, the inner product is distributive over vector addition, so this becomes
$$\sum_{k = 1}^n \lambda_k t_k \phi(\vec{x_k}) \cdot \phi(\vec{z}) + b^*.$$ At
this point, knowing a kernel, we can replace the inner product $\phi(\vec{x_k})
\cdot \phi(\vec{z})$ with $K(\vec{x}, \vec{z})$, where $K$ is a kernel of
$\phi$.

Equipped with the kernel, the only thing left to compute is compute $b^*$. This
computation only requires computing dot products $\phi(\vec{x_k}) \cdot
\phi(\vec{x_j})$, which can be replaced with kernel computations.

The book talks about another $K$, the Gram matrix, or the kernel of distances.
This is a confusing name and symbol; as far as I can tell it is not the actual
kernel. The book is remarkably silent on how this matrix works.

## The Karush--Kuhn--Tucker Method

If you were sick of the math _before_...

The Karush--Kuhn--Tucker method is used in support vector machines (SVMs) to
find the optimal linear classifier. It does this by minimizing
$\frac{1}{2}|\vec{w}|^2$ subject to $t_i y_i \geq 1$ for $i = 1, 2, \dots, n$.
The goal of KKT is to handle this inequality.

Before introducing KKT, we will describe the Lagrangian to see where KKT
differs.

### The Lagrangian

\newcommand{\lag}{\mathcal{L}}

We wish to minimize the function $f(\vec{x})$ subject to the list of
constraints $g_k(\vec{x}) = 0$, $k = 1, 2, \dots, n$. There are two
possibilities:

1. The gradient of $f$ is parallel to the gradient of each $g_k$, so that
moving along $g_k(\vec{x}) = 0$ will not decrease $f$; or

2. The gradient of $f$ is zero, so that $f$ is at a minimum.

The minima of $f$ will _possibly_ occur when one of these two conditions do.
These conditions can be succinctly summarized by defining the Lagrangian
$$\lag(\vec{x}, \vec{\lambda}) = f(\vec{x}) + \sum_{k = 1}^n \lambda_k
g_k(\vec{x}),$$ where $\vec{\lambda}$ is a vector of multiples meant to
describe all possibilities of parallel vectors. At critical points of $\lag$,
i.e. when $\nabla \lag = \vec{0}$, we have $$\frac{\partial \lag}{\partial
\lambda_k} = g_k(\vec{x}) = 0,$$ so that the constraints are satisfied, and
$$\nabla_{\vec{x}} f(\vec{x}) + \sum_{k = 1}^n \lambda_k \nabla_{\vec{x}}
g_k(\vec{x}) = \vec{0},$$ a technical condition that ensures that $\vec{x}$ is
a local minimum[^technicalities]. This gives us enough equations to completely
determine $\vec{x}$ and $\vec{\lambda}$, if such a solutions exists.

[^technicalities]: See [http://math.stackexchange.com/a/453421/261157](this
Math.SE answer). The rough sketch is that, for a point $\vec{x}$ to be a local
minimum for $f$ constrained by each $g_k$, then $\nabla f(\vec{x})$ must belong
to the vector space spanned by each $\nabla g_k$. Or, $\nabla f(\vec{x}) =
\sum_{k = 1}^n \lambda_k \nabla g_k(\vec{x})$, as stated.

In summary, _if a point is a minimum for the Lagrangian, it is a minimum for
$f$ subject to the given equality constraints._ Thus we solve the system given
by $\nabla \lag = \vec{0}$.

Note that this method only handles equality constraints, i.e. $g(\vec{x}) = 0$.
KKT will allow us to handle inequalities as well.

### KKT: Extending the Lagrangian with Inequalities

For KKT, we keep our equality restraints $g_k(\vec{x}) = 0$, but now add the
list of inequality restraints $$h_j(\vec{x}) \leq 0, \quad j = 1, 2, \dots,
m.$$ Just as we introduced the vector $\vec{\lambda}$ for equality constraints,
we will introduce the vector $\vec{\mu}$ for inequality restraints. Our
Lagrangian becomes $$\lag(\vec{x}, \vec{\lambda}, \vec{\mu}) = f(\vec{x}) +
\sum_{k = 1}^n \lambda_k g_k(\vec{x}) + \sum_{k = 1}^m \mu_k h_k(\vec{x}).$$

The procedure for this extended Lagrangian is identical to the original one for
$\vec{x}$. We find the critical points with respect to $\vec{x}$, giving
\begin{equation}
\label{kkt-stationary}
\nabla_{\vec{x}} f(\vec{x}) + \sum_{k = 1}^n \lambda_k \nabla_{\vec{x}}
g_k(\vec{x}) + \sum_{k = 1}^m \mu_k \nabla_{\vec{x}} h_k(\vec{x}) = \vec{0}.
\end{equation}
This ensures that $\vec{x}$ is a local minimum.

Next, we find the critical points with respect to $\vec{\lambda}$, but in a
slightly different way. For some reason^[Which I am not clear on at all.], the
solution $\vec{x}$ depends on $\vec{\lambda}$, so we may not treat functions of
$\vec{x}$ as constant with respect to $\vec{\lambda}$. Practically, this means
that we solve
\begin{equation}
\label{kkt-equality}
\nabla_{\vec{\lambda}} f(\vec{x}) + \sum_{k = 1}^n
\nabla_{\vec{\lambda}} (\lambda_k g_k(\vec{x})) + \sum_{k = 1}^m
\nabla_{\vec{\lambda}} (\mu_k h(\vec{x})) = \vec{0}.
\end{equation}

Finally, the inequality restraints are completely different. Instead of solving
$\nabla_{\vec{\mu}} \lag = 0$, for technical reasons, we have the following
requirements:
\begin{align}
\label{kkt-inequality}
\mu_j
h_j(\vec{x}) &= 0, \quad j = 1, 2, \dots, m \\
\mu_j &\geq 0. \label{kkt-nonnegative}
\end{align}

Solving the system given by \eqref{kkt-stationary}, \eqref{kkt-equality},
\eqref{kkt-inequality}, and \eqref{kkt-nonnegative} will give us possible
minima.

## SVM Algorithm Outline

After discussing the theoretical aspect of SVMs, we turn our attention to the
algorithm itself and its implementation. The implementation is provided by the
book, so we will mostly sketch the outline and discuss the software used.

The Python package `cvxopt` (convex optimization) is required for the book's
implementation. The free and open source package package numerically solves
convex optimization problems.  More details can be found at [the cvxopt
homepage](http://cvxopt.org/).

The algorithm is fairly straightforward, math aside. The authors choose to
support three different kernels: the linear, polynomial, and radial basis
function (RBF) kernels. There is some slight initialization to handle this
fact.

1. Compute the Gram matrix $K = XX^T$, where $X$ is the column vector
containing every datapoint.

- Linear kernel: return $K$.
- Polynomial kernel of degree $d$: return $\frac{K^d}{\sigma}$, where $\sigma$
  is a parameter.
- RBF: return $K_{ij} = \exp(-|x_i - x_j|^2 / \sigma^2)$.

2. Solve for $\vec{\lambda}$ using `cvxopt`.

We will use `cvxopt`'s quadratic solver, `cvxopt.solvers.qp()`. Its signature
is `cvxopt.solvers.qp(P, q, G, h, A, b)`, and it minimizes $$\frac{1}{2}
\vec{x} \cdot P\vec{x}$$ subject to $$G\vec{x} \leq \vec{h}, \quad A\vec{x} =
\vec{b}.$$ The text derives the appropriate values for these variables in terms
of the Gram matrix $K$ and $\vec{\lambda}$.

3. Compute $b^*$.

This completes the set up of the SVM. It is now ready to classify new
datapoints.
